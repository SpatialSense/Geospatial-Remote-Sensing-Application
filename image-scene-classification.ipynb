{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["\u003ca href=\"https://colab.research.google.com/github/JoKoum/satellite-image-classification/blob/main/satellite_image_classification.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"]},{"cell_type":"markdown","metadata":{"id":"Zg8rT9JV8_Dk"},"source":["# Project in Deep Learning Class: Satellite Image Classification\n","---\n","\n","### John Koumentis, Sotiris Panopoulos\n","---\n","\n","The project evaluates the performance of the encoder - decoder with attention mechanism architecture, over predicting the correct set of labels of the given satellite image chip."]},{"cell_type":"markdown","metadata":{"id":"UK_RQEep9fe0"},"source":["The JPG version of the satellite images dataset [*Planet: Understanding the Amazon from Space*](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space) was used. Those screenshots are chips extracted from the bigger dataset that are provided as a reference to the scene content."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2467,"status":"ok","timestamp":1724044203159,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"IRvDHaAcWl1x"},"outputs":[],"source":["%%capture\n","!gdown https://drive.google.com/uc?id=19U9KgKaqbl2NntvQfcQrPXYMNR3ZzxuL\n","!tar -xvf \"./train-jpg.tar\" -C \"./\""]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1724044205187,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"bLD1kC5hU22n"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import collections\n","from tqdm import tqdm\n","import random\n","import time\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","%matplotlib inline\n","from PIL import Image\n","from IPython.display import Image as Img\n","import tensorflow as tf\n","from sklearn.metrics import fbeta_score"]},{"cell_type":"markdown","metadata":{"id":"t_oVNQNsCr0S"},"source":["### Dataset\n","---\n","The dataset that maps the image names with the respective tags (labels) is read and modified, so as to inlude each image's whole filepath in Colab notebook. Additionally, another column that includes the respective labels as list items is created. Using that column, we extracted the unique labels existing at the dataset, and plotted a respective image per label."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"elapsed":969,"status":"error","timestamp":1724044215046,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"l1xtr1QjVHtY"},"outputs":[{"ename":"HTTPError","evalue":"HTTP Error 404: Not Found","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-3496ee8590d7\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/uc?id=1MsAf8Iktmf1dC1pYQpJ-QjC3lOAg5J_3\u0026export=download'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 718\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 372\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 274\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m\u003c=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m\u003c\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 557\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m\u003c=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m\u003c\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"]}],"source":["df = pd.read_csv('https://drive.google.com/uc?id=1MsAf8Iktmf1dC1pYQpJ-QjC3lOAg5J_3\u0026export=download')\n","df.head(10)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":5,"status":"error","timestamp":1724043022856,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"t0td-xKDU4hv","outputId":"172fc4e9-63ce-48da-edde-8affd81409c2"},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-6e8977e98f82\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./train-jpg/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags_split'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags_split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["df['image_name'] = './train-jpg/' + df['image_name'] + '.jpg'\n","\n","df['tags_split'] = df['tags'].apply(lambda x: x.split(' '))\n","labels_list = sum(list(df['tags_split'].values), [])\n","labels = set(labels_list)\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888804,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Np4xs3Hdt6z9"},"outputs":[],"source":["cnt_label = {}\n","for label_names in df['tags_split'].values:\n","  for l in label_names:\n","    cnt_label[l] = cnt_label[l] + 1 if l in cnt_label else 0\n","\n","plt.figure(figsize=(18,8))\n","idxs = range(len(cnt_label.values()))\n","plt.bar(idxs, cnt_label.values())\n","plt.xticks(idxs, cnt_label.keys(), rotation=-50)\n","plt.title('Labels Countplot')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888804,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"6XSk3A4oFGed"},"outputs":[],"source":["images_title = [df[df['tags'].str.contains(label)].iloc[i]['image_name'] for i, label in enumerate(labels)]\n","\n","_, ax = plt.subplots(5,4, figsize=(15,20))\n","ax = ax.ravel()\n","\n","for i, (image_name, label) in enumerate(zip(images_title, labels)):\n","  img = mpimg.imread(image_name)\n","  ax[i].imshow(img)\n","  ax[i].set_title('{}'.format(label))"]},{"cell_type":"markdown","metadata":{"id":"vEg0dX4lAY_V"},"source":["Details on the labels are present at [Chip (Image) Data Format](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data) section in Kaggle."]},{"cell_type":"markdown","metadata":{"id":"qXUDc8Jn5otZ"},"source":["### Image captioning approach\n","---\n","The [Image captioning with visual attention](https://www.tensorflow.org/tutorials/text/image_captioning) example was used as a base for the experiments that took place.\n","\n","As first step, we create a dictionary that maps each image path with its labels. In order to create a text sequence representation of labels, that is going to be used later, the \\\u003cstart\\\u003e and the \\\u003cend\\\u003e tokens have been added at the beginning and the end of each labels set."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888804,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"LscZE-u9x2E0"},"outputs":[],"source":["image_path_to_label = collections.defaultdict(list)\n","for image_path, label in zip(df['image_name'],df['tags'].values):\n","  image_path_to_label[image_path].append(f\"\u003cstart\u003e {label} \u003cend\u003e\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888805,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"jRqVnMGs0DgC"},"outputs":[],"source":["image_paths = list(image_path_to_label.keys())\n","train_labels = []\n","img_name_vector = []\n","\n","for image_path in image_paths:\n","  label_list = image_path_to_label[image_path]\n","  train_labels.extend(label_list)\n","  img_name_vector.extend([image_path]*len(label_list))\n","\n","print(len(img_name_vector))"]},{"cell_type":"markdown","metadata":{"id":"EIiLfrsZKU5T"},"source":["### Feature Extractor\n","---\n","A pre-trained Convolutional Neural Network was used, as feature extractor. The output of the last convolutional layer with shape (7, 7, 2048) will be used as the input feature for the Attention mechanism. ResNet50_V2 was chosen as Residual Networks are a state-of-the-art Convolutional Neural Network category that has been also used as feature extractor in similar tasks like the [\"Recurrent neural networks for remote sensing image classification\"](https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/iet-cvi.2017.0420) and [\"Self-attention for raw optical Satellite Time Series Classification\"](https://arxiv.org/pdf/1910.10536.pdf). Initial tests were also made using MobileNet architecture with similar results."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888805,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"E_ePRSiF6aOj"},"outputs":[],"source":["base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=(224,224,3),\n","                                               include_top=False,\n","                                               weights='imagenet')\n","\n","new_input = base_model.input\n","hidden_layer = base_model.layers[-1].output\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","tf.keras.utils.plot_model(base_model, to_file='feature_extractor.png', show_shapes=True)\n","Img(filename='feature_extractor.png')"]},{"cell_type":"markdown","metadata":{"id":"f1htXyrN4wp7"},"source":["### Image Preprocessing\n","---\n","We preprocess the images using the preprocess_input method to normalize the image so that it contains pixels with values in the range [-1, 1], which matches the format of the images used to train ResNet50_V2.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":683,"status":"ok","timestamp":1724044191307,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"ot7j-ZzP6rcL"},"outputs":[],"source":["def load_image(image_path):\n","    \"\"\"\n","    Input: Image path\n","    Output: Procesed image tensor, Image path\n","    \"\"\"\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (224, 224))\n","    img = tf.keras.applications.resnet_v2.preprocess_input(img)\n","    return img, image_path"]},{"cell_type":"markdown","metadata":{"id":"g-WwWMV5Kbvj"},"source":["Image features (last convolutional layer) are extracted and cached ((7 * 7 * 2048) floats per image) in NumPy binary file format, to be used during the netwoks training procedure."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888805,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"cLXGcPQ854Fh"},"outputs":[],"source":["encode_train = sorted(set(img_name_vector))\n","\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features,\n","                              (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())"]},{"cell_type":"markdown","metadata":{"id":"_aGW1guo92WQ"},"source":["### Tokenize labels\n","---\n","Using the TensorFlow Tokenizer method, labels are tokenized by getting split on spaces. Word to index and index to word mapping is created and finally, padding is applied, so as each created sequence to have the same length as the longest one."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888805,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"RzsFCmlU946-"},"outputs":[],"source":["def calc_max_length(tensor):\n","  \"\"\"\n","  Calculate max length of any labelset in the dataset\n","  \"\"\"\n","  return max(len(t) for t in tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"2ClQrL0Y-UPK"},"outputs":[],"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=len(set(train_labels)),\n","                                                  filters='!\"#$%\u0026()*+.,-/:;=?@[\\]^`{|}~')\n","tokenizer.fit_on_texts(train_labels)\n","\n","tokenizer.word_index['\u003cpad\u003e'] = 0\n","tokenizer.index_word[0] = '\u003cpad\u003e'\n","\n","train_sequences = tokenizer.texts_to_sequences(train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Pvq1NDAOPqSR"},"outputs":[],"source":["tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"DSPyXASj_eV7"},"outputs":[],"source":["# Pad each vector to the max_length of the labels\n","label_vector = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, padding='post')\n","print('Labels vector:\\n', label_vector)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"srqrW-69_u2f"},"outputs":[],"source":["max_length = calc_max_length(train_sequences)\n","print('Max sequence length: ',max_length)"]},{"cell_type":"markdown","metadata":{"id":"nWWmwJUFiXcf"},"source":["### Split to training and validation\n","---\n","Dataset is split into training and validation sets, 80% and 20% of the examples, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"ey51LjiSia6V"},"outputs":[],"source":["img_to_label_vector = collections.defaultdict(list)\n","\n","for image, label in zip(img_name_vector, label_vector):\n","  img_to_label_vector[image].append(label)\n","\n","img_keys = list(img_to_label_vector.keys())\n","random.shuffle(img_keys)\n","\n","slice_index = int(len(img_keys) *  0.8)\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","img_name_train = []\n","label_train = []\n","\n","for imgt in img_name_train_keys:\n","  label_len = len(img_to_label_vector[imgt])\n","  img_name_train.extend([imgt] * label_len)\n","  label_train.extend(img_to_label_vector[imgt])\n","\n","img_name_val = []\n","label_val = []\n","\n","for imgt in img_name_val_keys:\n","  label_len = len(img_to_label_vector[imgt])\n","  img_name_val.extend([imgt] * label_len)\n","  label_val.extend(img_to_label_vector[imgt])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"0yLv7B2dkg-G"},"outputs":[],"source":["# Print the respective training and validation lengths\n","len(img_name_train), len(label_train), len(img_name_val), len(label_val)"]},{"cell_type":"markdown","metadata":{"id":"uBcPa3zKlbsD"},"source":["### Create Dataset for training\n","---\n","Two datasets are created, one for the train and one for the validation set, using the TensorFlow Dataset module. Moreover, model hyperparameters are set up."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"vDgtDgsMlUOj"},"outputs":[],"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","vocab_size =  len(set(train_labels)) + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","num_steps_val = len(img_name_val) // BATCH_SIZE\n","# These two variables represent the feature vector shape\n","features_shape = 2048\n","attention_features_shape = 49"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"5QOJL89KmIow"},"outputs":[],"source":["def map_func(img_name, label):\n","  \"\"\"\n","  Load previously stored numpy files\n","  \"\"\"\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, label"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"YLBG0M1Hm0iC"},"outputs":[],"source":["dataset = tf.data.Dataset.from_tensor_slices((img_name_train, label_train))\n","\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888806,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"-kXaIf7_Z8sZ"},"outputs":[],"source":["val_set = tf.data.Dataset.from_tensor_slices((img_name_val, label_val))\n","\n","# Use map to load the numpy files in parallel\n","val_set = val_set.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Batch only\n","val_set = val_set.batch(BATCH_SIZE)\n","val_set = val_set.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"FCu9ZOtGjOiM"},"source":["### Model\n","---\n","Model architecture is inspired by the [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper. The authors propose an attention based model that automatically learns to describe the contents of images.\n","\n","We extract the features from the lower convolutional layer of ResNet50_v2 giving us a vector of shape (7, 7, 2048).\n","Then, it is flattened to a shape of (49, 2048). This vector is passed through the CNN Encoder. The RNN decoder attends over the image to predict the next word.\n","\n","The used attention mechanism is based on [Bahdanau's additive attention](https://arxiv.org/pdf/1409.0473.pdf). This frees the model from having to encode the whole input feature into a fixed-length vector, and lets the model focus only on information relevant to the generation of the next target word.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888807,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"dJ2QbNXijQjy"},"outputs":[],"source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","    # attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                         self.W2(hidden_with_time_axis)))\n","\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"7wZO4t-iwBdq"},"source":["CNN encoder reads the input sentence from the features previously extracted and encodes that information in vectors which are called hidden states. Since the convolution output (features) from the pretrained network have been extracted in advance, CNN encoder consists of a single fully connected layer. Performing the feature extraction during training could become a bottleneck."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888807,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"T7c7jrj8jUUJ"},"outputs":[],"source":["class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"3ShJZ44s0V-_"},"source":["RNN decoder produces a label set, by generating one word at every time step conditioned on a context vector, the previous hidden state and the\n","previously generated words.\n","\n","The decoder receives the complete encoder output.\n","It uses an RNN to keep track of what it has generated so far and it's RNN output as the query to the attention over the encoder's output, producing the context vector. Then, it combines the RNN output and the context vector to generate the \"attention vector\" and logit predictions for the next token based on the \"attention vector\".[[1]](https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_decoder)\n","\n","\n","Gated recurrent unit (GRU) layer was the one used at the decoder."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888807,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Sga-t8vHjXPS"},"outputs":[],"source":["class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))"]},{"cell_type":"markdown","metadata":{"id":"4Oqn6wm0zTA7"},"source":["The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input feature into a single fixed-length vector. Instead, it encodes the input into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the labels."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888807,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"ucOh9nlYjaPe"},"outputs":[],"source":["# Initialize encoder and decoder networks\n","\n","encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"e9-7Ah4jQBd3"},"source":["Initializing Adam optimizer and selecting Sparse Categorical Cross Entropy loss function.\n","\n","Adam optimizer was used, since it [*is computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters*](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n","\n","The Sparse Categorical Crossentropy Loss was used since the labels type is a vector of integers."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888807,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"9Bhe2zZ2a_Z3"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","#SP: for calculating accuracy during training\n","metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n","\n","train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"o9vsQLswjcNM"},"outputs":[],"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"sQ9vF4p6jffZ"},"outputs":[],"source":["# Create checkpoint to store trained model parameters for future use\n","checkpoint_path = \"./checkpoints\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"A9WZex8qjh-C"},"outputs":[],"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"sHPaiDQDjlT2"},"outputs":[],"source":["# List to keep the recorded training loss\n","train_loss_plot = []\n","val_loss_plot = []"]},{"cell_type":"markdown","metadata":{"id":"xsrgMELXJURj"},"source":["### Training pipeline\n","---\n","- The extracted features stored in the respective .npy files are passed through the CNN encoder.\n","- The encoder output, hidden state (initialized to 0) and the decoder input (which is the start token) is passed to the RNN decoder.\n","- The decoder returns the predictions and the decoder hidden state.\n","- The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","- Teacher forcing is used, to decide the next input to the decoder. Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n","- The final step is to calculate the gradients, apply them to the optimizer and backpropagate."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"L8jLj7_5jmfS"},"outputs":[],"source":["@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the labels are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['\u003cstart\u003e']] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          #train_acc_metric.update_state(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","\n","  return loss, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"hz7U7SI6N-Ej"},"outputs":[],"source":["@tf.function\n","def validation_step(img_tensor, target):\n","  loss = 0\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","  dec_input = tf.expand_dims([tokenizer.word_index['\u003cstart\u003e']] * target.shape[0], 1)\n","\n","  features = encoder(img_tensor)\n","\n","  for i in range(1, target.shape[1]):\n","    predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","    loss += loss_function(target[:, i], predictions)\n","    #val_acc_metric.update_state(target[:, i], predictions)\n","\n","    dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  return loss, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"V3-WXiPRjqFB"},"outputs":[],"source":["EPOCHS = 20\n","\n","val_loss_min = np.Inf\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","    val_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","\n","        if batch % 100 == 0:\n","            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n","\n","    # storing the epoch end loss value to plot later\n","    train_loss_plot.append(total_loss / num_steps)\n","\n","    # Display metrics at the end of each epoch.\n","   # train_acc = train_acc_metric.result()\n","    #print(\"Training accuracy over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    #train_acc_metric.reset_states()\n","\n","    for (batch, (img_tensor,target)) in enumerate(val_set):\n","\n","      batch_loss, v_loss = validation_step(img_tensor, target)\n","      val_loss += v_loss\n","\n","    val_loss_plot.append(val_loss/num_steps_val)\n","\n","    #val_acc = val_acc_metric.result()\n","    #val_acc_metric.reset_states()\n","    #print(\"Validation accuracy: %.4f\" % (float(val_acc),))\n","\n","    if val_loss \u003c val_loss_min:\n","      ckpt_manager.save()\n","      val_loss_min = val_loss\n","      print('Validation loss decreased, saving model')\n","\n","    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","    print(f'Validation loss {val_loss/num_steps_val:.6f}')\n","    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"vQ-of37yznXO"},"outputs":[],"source":["plt.figure(figsize=(15,15))\n","plt.plot(train_loss_plot, label='Train Loss')\n","plt.plot(val_loss_plot, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8_V_uQD8MZfm"},"source":["Summary of the encoder - decoder architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"xB5A-xuJMYyQ"},"outputs":[],"source":["encoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888808,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"HwdSe3ERMaIJ"},"outputs":[],"source":["decoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"FNWk2HXDK1wv"},"source":["### Evaluation procedure\n","---\n","The model is reset to the state where validation loss had the lowest value.\n","\n","The average F-beta score is calculated within the validation dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Novyv-DqbDVu"},"outputs":[],"source":["def test(image):\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n","                                                 -1,\n","                                                 img_tensor_val.shape[3]))\n","\n","    features = encoder(img_tensor_val)\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['\u003cstart\u003e']], 0)\n","    result = []\n","    flag = False\n","\n","    for i in range(max_length-1):\n","        if not flag:\n","          predictions, hidden, _ = decoder(dec_input,features, hidden)\n","\n","\n","          predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","          #result.append(tokenizer.index_word[predicted_id])\n","          result.append(predicted_id)\n","\n","          if tokenizer.index_word[predicted_id] == '\u003cend\u003e':\n","              flag = True\n","        else:\n","          result.append(0)\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"o_AOnsdIbC7A"},"outputs":[],"source":["def evaluation_score():\n","  score = 0\n","  for id in range(0, len(img_name_val)-1):\n","    image = img_name_val[id]\n","    real_caption = [i for i in label_val[id] if i != 1]\n","    real = np.sum(tf.one_hot(real_caption, len(tokenizer.word_index)).numpy(), axis=0)\n","\n","    result = test(image)\n","    predicted = np.sum(tf.one_hot(result,len(tokenizer.word_index)).numpy(), axis=0)\n","\n","    f = fbeta_score(real, predicted, average='weighted', beta=1)\n","\n","    score += f/len(img_name_val)\n","\n","  return score"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Sr6XJLiHbNHt"},"outputs":[],"source":["score = evaluation_score()\n","print('Average F-beta score at validation dataset: ', score)"]},{"cell_type":"markdown","metadata":{"id":"bSZQp1APrVtI"},"source":["The attention per image label is plotted along with the image chip, to show graphically the image region where the model 'looked' so as to output the label."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"hitGbk1SOqO7"},"outputs":[],"source":["def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n","                                                 -1,\n","                                                 img_tensor_val.shape[3]))\n","\n","    features = encoder(img_tensor_val)\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['\u003cstart\u003e']], 0)\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input,\n","                                                         features,\n","                                                         hidden)\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '\u003cend\u003e':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"dc3pUa-GOyG-"},"outputs":[],"source":["def plot_attention(image, result, attention_plot):\n","    image = Image.open(image)\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for i in range(len_result):\n","        temp_att = np.resize(attention_plot[i], (8, 8))\n","        grid_size = max(np.ceil(len_result/2), 2)\n","        ax = fig.add_subplot(grid_size, grid_size, i+1)\n","        ax.set_title(result[i])\n","        img = ax.imshow(image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"K7gkIXHTZYag"},"outputs":[],"source":["def test_image(img_id):\n","  # Labels on the validation set\n","  image = img_name_val[img_id]\n","  real_caption = ' '.join([tokenizer.index_word[i] for i in label_val[img_id] if i not in [0]])\n","  result, attention_plot = evaluate(image)\n","\n","  actual_image = Image.open(image)\n","  plt.imshow(actual_image)\n","\n","  print('Real Caption:', real_caption)\n","  print('Prediction Caption:', ' '.join(result))\n","  plot_attention(image, result, attention_plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"T5hLfEjHCLXO"},"outputs":[],"source":["# Restore networks at their latest saved state\n","ckpt.restore(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"8r1muNNfFTIa"},"outputs":[],"source":["!zip -r './checkpoints.zip' './checkpoints'"]},{"cell_type":"markdown","metadata":{"id":"YZS8y1XR6Nyg"},"source":["Evaluating the performance on 8 chip images from the validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"U1GlZ7KfO0eV"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47,"status":"aborted","timestamp":1724042888809,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"S8-jnvCZe7H5"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"-sWz4R_rjYEw"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"iJg859v46G9N"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"Ey14GH5u6KIh"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"vO7jHe4a6XVy"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"tr4af2Vb6Yui"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1724042888810,"user":{"displayName":"Spatial Sense","userId":"07960817545296539462"},"user_tz":-330},"id":"6a7HVkut6ZdE"},"outputs":[],"source":["id = np.random.randint(0, len(img_name_val))\n","test_image(id)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["7jYTRkjP5iAk","Sj8QC8yHnCf-"],"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}